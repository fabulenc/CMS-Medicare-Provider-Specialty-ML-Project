{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to tune: l1 and/or l2 reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Subsampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m groups \u001b[38;5;241m=\u001b[39m groups\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     11\u001b[0m y_columns \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m---> 13\u001b[0m le \u001b[38;5;241m=\u001b[39m \u001b[43mLabelEncoder\u001b[49m()\n\u001b[1;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir('../data')\n",
    "\n",
    "X = pd.read_csv('X_3specialties_equalWeight_subsample.zip',compression='zip', index_col=False)\n",
    "y = pd.read_csv('y_3specialties_equalWeight_subsample.zip',compression='zip')\n",
    "groups = pd.read_csv('groups_3specialties_equalWeight_subsample.zip',compression='zip')\n",
    "\n",
    "X = X.iloc[:,1:]\n",
    "y = y.iloc[:,1:]\n",
    "groups = groups.iloc[:,1:]\n",
    "\n",
    "y_columns = y.columns\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = y.values.ravel()\n",
    "y = le.fit_transform(y)\n",
    "y = pd.DataFrame(y)\n",
    "y.columns = y_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ftrs = ['Prscrbr_City',\n",
    "                    'Prscrbr_State_Abrvtn',\n",
    "                    'Brnd_Name',\n",
    "                    'Gnrc_Name']\n",
    "\n",
    "std_ftrs = ['Tot_Clms', \n",
    "            'Tot_30day_Fills', \n",
    "            'Tot_Day_Suply', \n",
    "            'Tot_Drug_Cst', \n",
    "            'Tot_Benes', \n",
    "            'GE65_Tot_Clms',\n",
    "            'GE65_Tot_30day_Fills',\n",
    "            'GE65_Tot_Drug_Cst',\n",
    "            'GE65_Tot_Day_Suply',\n",
    "            'GE65_Tot_Benes']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), categorical_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prscrbr_City              Yuba City\n",
       "Prscrbr_State_Abrvtn             CA\n",
       "Brnd_Name               Allopurinol\n",
       "Gnrc_Name               Allopurinol\n",
       "Tot_Clms                         35\n",
       "Tot_30day_Fills                87.0\n",
       "Tot_Day_Suply                  2610\n",
       "Tot_Drug_Cst                 505.25\n",
       "Tot_Benes                       NaN\n",
       "GE65_Tot_Clms                   NaN\n",
       "GE65_Tot_30day_Fills            NaN\n",
       "GE65_Tot_Drug_Cst               NaN\n",
       "GE65_Tot_Day_Suply              NaN\n",
       "GE65_Tot_Benes                  NaN\n",
       "Name: 32, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_subsample2.iloc[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid1 = {'solver': ['saga'],\n",
    "    'penalty' : ['none']}   #'solver'  : ['newton-cg', 'lbfgs', 'liblinear']\n",
    "\n",
    "\n",
    "param_grid2 = {'solver': ['saga'],\n",
    "               'penalty' : ['l1','l2'], \n",
    "               'C'       : np.logspace(-3,3,7)}   #'solver'  : ['newton-cg', 'lbfgs', 'liblinear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2,2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer w/ Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n%%time\\n\\n## Perform 60-20-20 Split\\n# splitter for subsampled data\\nstratGroupKFold2 = StratifiedGroupKFold(n_splits=5)\\n\\n# splitter for other\\nstratGroupKFold3 = StratifiedGroupKFold(n_splits=4)\\n\\n\\nnr_states = [0] #,1,2\\ncounter = 0\\ncounter2 = 0\\nfinal_models = []\\n\\n\\nfor i_other2,i_test2 in stratGroupKFold2.split(X_subsample2.values, y_subsample2, groups_subsample2.values):\\n    \\n    counter = counter + 1\\n    \\n    X_other2, y_other2, groups_other2 = X_subsample2.iloc[i_other2], y_subsample2.iloc[i_other2], groups_subsample2.iloc[i_other2]\\n    X_test2, y_test2, groups_test2 = X_subsample2.iloc[i_test2], y_subsample2.iloc[i_test2], groups_subsample2.iloc[i_test2]\\n    \\n    \\n    print(f\\'Test Set #{counter}\\')\\n\\n    print(\"    Test Set Size:\", len(y_test2))\\n\\n    print()\\n    \\n    #for i in range(len(nr_states)):\\n\\n        #print(\"         Random State:\", i)\\n        #print()\\n        \\n    counter2 = 0\\n    \\n    for i in range(len(nr_states)):\\n    \\n        for i_train3,i_val3 in stratGroupKFold3.split(X_other2.values, y_other2.values, groups_other2.values):\\n            counter2 = counter2 + 1\\n\\n            # Perform n-Fold CV\\n            #cv = stratGroupKFold3.split(X_other2, y_other2, groups_other2)\\n\\n            X_train, y_train, groups_train = X_other2.iloc[i_train3], y_other2.iloc[i_train3], groups_other2.iloc[i_train3]\\n            X_val, y_val, groups_val = X_other2.iloc[i_val3], y_other2.iloc[i_val3], groups_other2.iloc[i_val3]\\n\\n            print(f\"    Train Set #{counter2} Size: {len(X_train)}\")\\n            print()\\n\\n\\n            X_prep = preprocessor.fit_transform(X_train)\\n\\n            # collect feature names\\n            feature_names = preprocessor.get_feature_names_out()\\n\\n            df_train = pd.DataFrame(data=X_prep,columns=feature_names)\\n            print(f\"    Train Set Shape after preprocessing: {df_train.shape}\")\\n            print()\\n\\n            # transform the CV\\n            df_CV = preprocessor.transform(X_val)\\n            df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\\n            print(f\"    Validation Set Shape after preprocessing: {df_CV.shape}\")\\n            print()\\n\\n\\n            # transform the test\\n            df_test = preprocessor.transform(X_test2)\\n            df_test = pd.DataFrame(data=df_test,columns = feature_names)\\n            print(f\"    Test Set Shape after preprocessing: {df_test.shape}\")\\n            print()\\n\\n\\n            ### Multivariate Imputation\\n            imputer = IterativeImputer(estimator = LinearRegression(), random_state=i)\\n            X_impute = imputer.fit_transform(df_train)\\n            df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\\n\\n\\n            df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\\n            df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)\\n            \\n            print(\\'Imputation Step ----> Done\\')\\n\\n\\n            ### Logistic Regression\\n            #logreg = LogisticRegression()\\n\\n            #grid = GridSearchCV(logreg,                    # model\\n                       #param_grid = parameters,   # hyperparameters\\n                       #scoring=\\'accuracy\\')\\n\\n            #grid.fit(df_CV_imp, y_val, groups = groups_val, )\\n\\n\\n            train_score = np.zeros(len(ParameterGrid(param_grid)))\\n            val_score = np.zeros(len(ParameterGrid(param_grid)))\\n            models = []\\n\\n            # loop through all combinations of hyperparameter combos\\n            for p in range(len(ParameterGrid(param_grid))):\\n                params = ParameterGrid(param_grid1)[p]\\n                print(\\'   \\',params) \\n                clf = LogisticRegression(**params, random_state = i, n_jobs=1) # initialize the classifier\\n                clf.fit(df_train_imp,y_train) # fit the model\\n                models.append(clf) # save it\\n                # calculate train and validation accuracy scores\\n                y_train_pred = clf.predict(df_train_imp)\\n                train_score[p] = accuracy_score(y_train,y_train_pred)\\n                y_val_pred = clf.predict(df_CV_imp)\\n                val_score[p] = accuracy_score(y_val,y_val_pred)\\n                print(\\'   \\',train_score[p],val_score[p])\\n\\n            # print out model parameters that maximize validation accuracy\\n            print(\\'best model parameters:\\',ParameterGrid(param_grid)[np.argmax(val_score)])\\n            print(\\'corresponding validation score:\\',np.max(val_score))\\n            # collect and save the best model\\n            final_models.append(models[np.argmax(val_score)])\\n            # calculate and save the test score\\n            #y_test_pred = final_models[-1].predict(X_test_prep)\\n            #test_scores[i] = accuracy_score(y_test,y_test_pred)\\n            #print(\\'test score:\\',test_scores[i])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "\n",
    "## Perform 60-20-20 Split\n",
    "# splitter for subsampled data\n",
    "stratGroupKFold2 = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "# splitter for other\n",
    "stratGroupKFold3 = StratifiedGroupKFold(n_splits=4)\n",
    "\n",
    "\n",
    "nr_states = [0] #,1,2\n",
    "counter = 0\n",
    "counter2 = 0\n",
    "final_models = []\n",
    "\n",
    "\n",
    "for i_other2,i_test2 in stratGroupKFold2.split(X_subsample2.values, y_subsample2, groups_subsample2.values):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_other2, y_other2, groups_other2 = X_subsample2.iloc[i_other2], y_subsample2.iloc[i_other2], groups_subsample2.iloc[i_other2]\n",
    "    X_test2, y_test2, groups_test2 = X_subsample2.iloc[i_test2], y_subsample2.iloc[i_test2], groups_subsample2.iloc[i_test2]\n",
    "    \n",
    "    \n",
    "    print(f'Test Set #{counter}')\n",
    "\n",
    "    print(\"    Test Set Size:\", len(y_test2))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    #for i in range(len(nr_states)):\n",
    "\n",
    "        #print(\"         Random State:\", i)\n",
    "        #print()\n",
    "        \n",
    "    counter2 = 0\n",
    "    \n",
    "    for i in range(len(nr_states)):\n",
    "    \n",
    "        for i_train3,i_val3 in stratGroupKFold3.split(X_other2.values, y_other2.values, groups_other2.values):\n",
    "            counter2 = counter2 + 1\n",
    "\n",
    "            # Perform n-Fold CV\n",
    "            #cv = stratGroupKFold3.split(X_other2, y_other2, groups_other2)\n",
    "\n",
    "            X_train, y_train, groups_train = X_other2.iloc[i_train3], y_other2.iloc[i_train3], groups_other2.iloc[i_train3]\n",
    "            X_val, y_val, groups_val = X_other2.iloc[i_val3], y_other2.iloc[i_val3], groups_other2.iloc[i_val3]\n",
    "\n",
    "            print(f\"    Train Set #{counter2} Size: {len(X_train)}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            X_prep = preprocessor.fit_transform(X_train)\n",
    "\n",
    "            # collect feature names\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "            df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "            print(f\"    Train Set Shape after preprocessing: {df_train.shape}\")\n",
    "            print()\n",
    "\n",
    "            # transform the CV\n",
    "            df_CV = preprocessor.transform(X_val)\n",
    "            df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "            print(f\"    Validation Set Shape after preprocessing: {df_CV.shape}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            # transform the test\n",
    "            df_test = preprocessor.transform(X_test2)\n",
    "            df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "            print(f\"    Test Set Shape after preprocessing: {df_test.shape}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            ### Multivariate Imputation\n",
    "            imputer = IterativeImputer(estimator = LinearRegression(), random_state=i)\n",
    "            X_impute = imputer.fit_transform(df_train)\n",
    "            df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "\n",
    "            df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "            df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)\n",
    "            \n",
    "            print('Imputation Step ----> Done')\n",
    "\n",
    "\n",
    "            ### Logistic Regression\n",
    "            #logreg = LogisticRegression()\n",
    "\n",
    "            #grid = GridSearchCV(logreg,                    # model\n",
    "                       #param_grid = parameters,   # hyperparameters\n",
    "                       #scoring='accuracy')\n",
    "\n",
    "            #grid.fit(df_CV_imp, y_val, groups = groups_val, )\n",
    "\n",
    "\n",
    "            train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "            val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "            models = []\n",
    "\n",
    "            # loop through all combinations of hyperparameter combos\n",
    "            for p in range(len(ParameterGrid(param_grid))):\n",
    "                params = ParameterGrid(param_grid1)[p]\n",
    "                print('   ',params) \n",
    "                clf = LogisticRegression(**params, random_state = i, n_jobs=1) # initialize the classifier\n",
    "                clf.fit(df_train_imp,y_train) # fit the model\n",
    "                models.append(clf) # save it\n",
    "                # calculate train and validation accuracy scores\n",
    "                y_train_pred = clf.predict(df_train_imp)\n",
    "                train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "                y_val_pred = clf.predict(df_CV_imp)\n",
    "                val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "                print('   ',train_score[p],val_score[p])\n",
    "\n",
    "            # print out model parameters that maximize validation accuracy\n",
    "            print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "            print('corresponding validation score:',np.max(val_score))\n",
    "            # collect and save the best model\n",
    "            final_models.append(models[np.argmax(val_score)])\n",
    "            # calculate and save the test score\n",
    "            #y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "            #test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "            #print('test score:',test_scores[i])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_pipeline_ImputationLogisticReg_GridSearchCV(X, y, groups, i):\n",
    "    # create a test set based on groups\n",
    "    splitter = GroupShuffleSplit(n_splits=1,test_size=0.2,random_state=i)\n",
    "    \n",
    "    # Get Test Set\n",
    "    for i_other,i_test in splitter.split(X, y, groups):\n",
    "        X_other, y_other, groups_other = X.iloc[i_other], y.iloc[i_other], groups.iloc[i_other]\n",
    "        X_test, y_test, groups_test = X.iloc[i_test], y.iloc[i_test], groups.iloc[i_test]\n",
    "    \n",
    "    # Get Validation Set\n",
    "    \n",
    "    kf = GroupKFold(n_splits=5)\n",
    "    counter = 0\n",
    "    for i_train, i_test in kf.split(X_other, y_other, groups_other):\n",
    "        X_train, y_train, groups_train = X_other.iloc[i_train], y_other.iloc[i_train], groups_other.iloc[i_train]\n",
    "        X_val, y_val, groups_val = X_other.iloc[i_test], y_other.iloc[i_test], groups_other.iloc[i_test]\n",
    "        \n",
    "        #print(len(y_val))\n",
    "        #print(len(y_train))\n",
    "        counter = counter + 1\n",
    "        \n",
    "        print(f\"CV # {counter}\")\n",
    "        X_prep = preprocessor.fit_transform(X_train)\n",
    "        # collect feature names\n",
    "        \n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "        \n",
    "        df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "        print(f\"Train Set Shape (after preprocessing): {df_train.shape}\")\n",
    "        print()\n",
    "        \n",
    "        # transform the CV\n",
    "        df_CV = preprocessor.transform(X_val)\n",
    "        df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "        print(f\"CV Set Shape (after preprocessing): {df_CV.shape}\")\n",
    "        print()\n",
    "        # transform the test\n",
    "        df_test = preprocessor.transform(X_test)\n",
    "        df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "        print(f\"Test Set Shape (after preprocessing): {df_test.shape}\")\n",
    "        print()\n",
    "        \n",
    "        y_CV = y_val\n",
    "        \n",
    "        \n",
    "        ### Multivariate Imputation\n",
    "        imputer = IterativeImputer(estimator = LinearRegression(), random_state=i)\n",
    "        X_impute = imputer.fit_transform(df_train)\n",
    "        df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "\n",
    "        df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "        df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)\n",
    "\n",
    "        train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "        val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "        models = []\n",
    "        \n",
    "        print(\"*** Imputation Done ***\")\n",
    "        print()\n",
    "\n",
    "        # loop through all combinations of hyperparameter combos\n",
    "        for p in range(len(ParameterGrid(param_grid1))):\n",
    "            params = ParameterGrid(param_grid1)[p]\n",
    "            print('   ',params) \n",
    "            clf = LogisticRegression(**params, random_state = i, n_jobs=1) # initialize the classifier\n",
    "            clf.fit(df_train_imp,y_train) # fit the model\n",
    "            models.append(clf) # save it\n",
    "            # calculate train and validation accuracy scores\n",
    "            y_train_pred = clf.predict(df_train_imp)\n",
    "            train_score[p] = f1_score(y_train,y_train_pred, average = 'macro')\n",
    "            y_val_pred = clf.predict(df_CV_imp)\n",
    "            val_score[p] = f1_score(y_val,y_val_pred, average = 'macro')\n",
    "            print('   ',train_score[p],val_score[p])\n",
    "\n",
    "        # print out model parameters that maximize validation accuracy\n",
    "        print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "        print('corresponding validation score:',np.max(val_score))\n",
    "        # collect and save the best model\n",
    "        final_models.append(models[np.argmax(val_score)])\n",
    "        # calculate and save the test score\n",
    "        y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "        acc = accuracy_score(y_test,y_test_pred)\n",
    "        test_score = f1_score(y_test,y_test_pred, average = 'macro')\n",
    "        print('f1 score:',test_scores)\n",
    "        print('accuracy score:',acc)\n",
    "        #total_accuracy, f1, cm = reduced_feature_xgb(df_train, y_train, df_CV, y_CV, df_test, y_test, i)\n",
    "        cm = confusion_matrix(y_test,y_test_pred)\n",
    "        return test_score, acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State # 0\n",
      "\n",
      "CV # 1\n",
      "Train Set Shape (after preprocessing): (2808, 856)\n",
      "\n",
      "CV Set Shape (after preprocessing): (702, 856)\n",
      "\n",
      "Test Set Shape (after preprocessing): (694, 856)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:12\u001b[0m\n",
      "Cell \u001b[0;32mIn [10], line 48\u001b[0m, in \u001b[0;36mML_pipeline_ImputationLogisticReg_GridSearchCV\u001b[0;34m(X, y, groups, i)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m### Multivariate Imputation\u001b[39;00m\n\u001b[1;32m     47\u001b[0m imputer \u001b[38;5;241m=\u001b[39m IterativeImputer(estimator \u001b[38;5;241m=\u001b[39m LinearRegression(), random_state\u001b[38;5;241m=\u001b[39mi)\n\u001b[0;32m---> 48\u001b[0m X_impute \u001b[38;5;241m=\u001b[39m \u001b[43mimputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m df_train_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mX_impute, columns \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m     52\u001b[0m df_CV_imp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mimputer\u001b[38;5;241m.\u001b[39mtransform(df_CV), columns \u001b[38;5;241m=\u001b[39m df_train\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/impute/_iterative.py:665\u001b[0m, in \u001b[0;36mIterativeImputer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feat_idx \u001b[38;5;129;01min\u001b[39;00m ordered_idx:\n\u001b[1;32m    662\u001b[0m     neighbor_feat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_neighbor_feat_idx(\n\u001b[1;32m    663\u001b[0m         n_features, feat_idx, abs_corr_mat\n\u001b[1;32m    664\u001b[0m     )\n\u001b[0;32m--> 665\u001b[0m     Xt, estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impute_one_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_missing_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneighbor_feat_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    673\u001b[0m     estimator_triplet \u001b[38;5;241m=\u001b[39m _ImputerTriplet(\n\u001b[1;32m    674\u001b[0m         feat_idx, neighbor_feat_idx, estimator\n\u001b[1;32m    675\u001b[0m     )\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputation_sequence_\u001b[38;5;241m.\u001b[39mappend(estimator_triplet)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/impute/_iterative.py:318\u001b[0m, in \u001b[0;36mIterativeImputer._impute_one_feature\u001b[0;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[1;32m    316\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m _safe_indexing(X_filled[:, neighbor_feat_idx], \u001b[38;5;241m~\u001b[39mmissing_row_mask)\n\u001b[1;32m    317\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m _safe_indexing(X_filled[:, feat_idx], \u001b[38;5;241m~\u001b[39mmissing_row_mask)\n\u001b[0;32m--> 318\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# if no missing values, don't predict\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(missing_row_mask) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/sklearn/linear_model/_base.py:736\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outs])\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, _, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingular_ \u001b[38;5;241m=\u001b[39m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/scipy/linalg/_basic.py:1192\u001b[0m, in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m real_data:\n\u001b[1;32m   1191\u001b[0m     lwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n, nrhs, cond)\n\u001b[0;32m-> 1192\u001b[0m     x, s, rank, info \u001b[38;5;241m=\u001b[39m \u001b[43mlapack_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43miwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# complex data\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     lwork, rwork, iwork \u001b[38;5;241m=\u001b[39m _compute_lwork(lapack_lwork, m, n,\n\u001b[1;32m   1196\u001b[0m                                          nrhs, cond)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "final_models = []\n",
    "test_scores = []\n",
    "best_params = []\n",
    "confusion_mat = []\n",
    "class_met = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(f'Random State # {i}')\n",
    "    print()\n",
    "    \n",
    "    test_score, acc, cm = ML_pipeline_ImputationLogisticReg_GridSearchCV(X, y, groups, i)\n",
    "    \n",
    "    confusion_mat.append(cm)\n",
    "    \n",
    "    #class_met.append(class_metrics)\n",
    "    \n",
    "    print()\n",
    "    print('test score:', test_score)\n",
    "    test_scores.append(test_score)\n",
    "    print()\n",
    "    accuracy_scores.append(acc)\n",
    "    \n",
    "print('test accuracy:',np.around(np.mean(test_scores),2),'+/-',np.around(np.std(test_scores),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer w/ RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "imputer = IterativeImputer(estimator = RandomForestRegressor(n_estimators=1), random_state=42)\n",
    "\n",
    "\n",
    "%%time\n",
    "\n",
    "## Perform 60-20-20 Split\n",
    "# splitter for subsampled data\n",
    "stratGroupKFold2 = StratifiedGroupKFold(n_splits=5)\n",
    "\n",
    "# splitter for other\n",
    "stratGroupKFold3 = StratifiedGroupKFold(n_splits=4)\n",
    "\n",
    "\n",
    "nr_states = [0] #,1,2\n",
    "counter = 0\n",
    "counter2 = 0\n",
    "final_models = []\n",
    "\n",
    "\n",
    "for i_other2,i_test2 in stratGroupKFold2.split(X_subsample2.values, y_subsample2, groups_subsample2.values):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_other2, y_other2, groups_other2 = X_subsample2.iloc[i_other2], y_subsample2.iloc[i_other2], groups_subsample2.iloc[i_other2]\n",
    "    X_test2, y_test2, groups_test2 = X_subsample2.iloc[i_test2], y_subsample2.iloc[i_test2], groups_subsample2.iloc[i_test2]\n",
    "    \n",
    "    \n",
    "    print(f'Test Set #{counter}')\n",
    "\n",
    "    print(\"    Test Set Size:\", len(y_test2))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    #for i in range(len(nr_states)):\n",
    "\n",
    "        #print(\"         Random State:\", i)\n",
    "        #print()\n",
    "        \n",
    "    counter2 = 0\n",
    "    \n",
    "    for i in range(len(nr_states)):\n",
    "    \n",
    "        for i_train3,i_val3 in stratGroupKFold3.split(X_other2.values, y_other2.values, groups_other2.values):\n",
    "            counter2 = counter2 + 1\n",
    "\n",
    "            # Perform n-Fold CV\n",
    "            #cv = stratGroupKFold3.split(X_other2, y_other2, groups_other2)\n",
    "\n",
    "            X_train, y_train, groups_train = X_subsample2.iloc[i_train3], y_subsample2.iloc[i_train3], groups_subsample2.iloc[i_train3]\n",
    "            X_val, y_val, groups_val = X_subsample2.iloc[i_val3], y_subsample2.iloc[i_val3], groups_subsample2.iloc[i_val3]\n",
    "\n",
    "            print(f\"    Train Set #{counter2} Size: {len(X_train)}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            X_prep = preprocessor.fit_transform(X_train)\n",
    "\n",
    "            # collect feature names\n",
    "            feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "            df_train = pd.DataFrame(data=X_prep,columns=feature_names)\n",
    "            print(f\"    Train Set Shape after preprocessing: {df_train.shape}\")\n",
    "            print()\n",
    "\n",
    "            # transform the CV\n",
    "            df_CV = preprocessor.transform(X_val)\n",
    "            df_CV = pd.DataFrame(data=df_CV,columns = feature_names)\n",
    "            print(f\"    Validation Set Shape after preprocessing: {df_CV.shape}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            # transform the test\n",
    "            df_test = preprocessor.transform(X_test2)\n",
    "            df_test = pd.DataFrame(data=df_test,columns = feature_names)\n",
    "            print(f\"    Test Set Shape after preprocessing: {df_test.shape}\")\n",
    "            print()\n",
    "\n",
    "\n",
    "            ### Multivariate Imputation\n",
    "            imputer = IterativeImputer(estimator = LinearRegression(), random_state=i)\n",
    "            X_impute = imputer.fit_transform(df_train)\n",
    "            df_train_imp = pd.DataFrame(data=X_impute, columns = df_train.columns)\n",
    "\n",
    "\n",
    "            df_CV_imp = pd.DataFrame(data=imputer.transform(df_CV), columns = df_train.columns)\n",
    "            df_test_imp = pd.DataFrame(data=imputer.transform(df_test), columns = df_train.columns)\n",
    "            \n",
    "            print('    *** Imputation Step Done *** ')\n",
    "\n",
    "\n",
    "            ### Logistic Regression\n",
    "            #logreg = LogisticRegression()\n",
    "\n",
    "            #grid = GridSearchCV(logreg,                    # model\n",
    "                       #param_grid = parameters,   # hyperparameters\n",
    "                       #scoring='accuracy')\n",
    "\n",
    "            #grid.fit(df_CV_imp, y_val, groups = groups_val, )\n",
    "\n",
    "\n",
    "            train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "            val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "            models = []\n",
    "\n",
    "            # loop through all combinations of hyperparameter combos\n",
    "            for p in range(len(ParameterGrid(param_grid))):\n",
    "                params = ParameterGrid(param_grid)[p]\n",
    "                print('   ',params) \n",
    "                clf = LogisticRegression(**params, random_state = i, n_jobs=1) # initialize the classifier\n",
    "                clf.fit(df_train_imp,y_train) # fit the model\n",
    "                models.append(clf) # save it\n",
    "                # calculate train and validation accuracy scores\n",
    "                y_train_pred = clf.predict(df_train_imp)\n",
    "                train_score[p] = accuracy_score(y_train,y_train_pred)\n",
    "                y_val_pred = clf.predict(df_CV_imp)\n",
    "                val_score[p] = accuracy_score(y_val,y_val_pred)\n",
    "                print('   ',train_score[p],val_score[p])\n",
    "\n",
    "            # print out model parameters that maximize validation accuracy\n",
    "            print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "            print('corresponding validation score:',np.max(val_score))\n",
    "            # collect and save the best model\n",
    "            final_models.append(models[np.argmax(val_score)])\n",
    "            # calculate and save the test score\n",
    "            #y_test_pred = final_models[-1].predict(X_test_prep)\n",
    "            #test_scores[i] = accuracy_score(y_test,y_test_pred)\n",
    "            #print('test score:',test_scores[i])\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
