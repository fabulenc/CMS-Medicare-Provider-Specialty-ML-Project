{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_ftrs = ['Prscrbr_City',\n",
    "                    'Prscrbr_State_Abrvtn',\n",
    "                    'Brnd_Name',\n",
    "                    'Gnrc_Name']\n",
    "\n",
    "std_ftrs = ['Tot_Clms', \n",
    "            'Tot_30day_Fills', \n",
    "            'Tot_Day_Suply', \n",
    "            'Tot_Drug_Cst', \n",
    "            'Tot_Benes', \n",
    "            'GE65_Tot_Clms',\n",
    "            'GE65_Tot_30day_Fills',\n",
    "            'GE65_Tot_Drug_Cst',\n",
    "            'GE65_Tot_Day_Suply',\n",
    "            'GE65_Tot_Benes']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), categorical_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)])                                               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../data')\n",
    "\n",
    "X_subsample2 = pd.read_csv('X_subsample_round1_split2.zip',compression='zip',index_col=False)\n",
    "y_subsample2 = pd.read_csv('y_subsample_round1_split2.zip',compression='zip')\n",
    "groups_subsample2 = pd.read_csv('groups_subsample_round1_split2.zip',compression='zip')\n",
    "\n",
    "X_subsample2 = X_subsample2.iloc[:,1:]\n",
    "y_subsample2 = y_subsample2.iloc[:,1:]\n",
    "groups_subsample2 = groups_subsample2.iloc[:,1:]\n",
    "\n",
    "y_subsample2_columns = y_subsample2.columns\n",
    "#y_subsample = y_subsample.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample [0.4, 0.6, 0.8]\n",
    "# max depth [1, 3, 10, 30]\n",
    "# lr [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "param_grid = {\"xgbclassifier__subsample\": [0.4, 0.6, 0.8],\n",
    "              \"xgbclassifier__missing\": [np.nan],\n",
    "              \"xgbclassifier__max_depth\": [1, 3, 10, 30],\n",
    "              \"xgbclassifier__learning_rate\": [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "              \"xgbclassifier__n_estimators\": [500],\n",
    "              \"xgbclassifier__colsample_bytree\": [0.4, 0.6, 0.8]}\n",
    "\n",
    "\n",
    "param_grid1 = {\"xgbclassifier__subsample\": [0.4],\n",
    "              \"xgbclassifier__missing\": [np.nan],\n",
    "              \"xgbclassifier__max_depth\": [1],\n",
    "              \"xgbclassifier__learning_rate\": [0.01],\n",
    "              \"xgbclassifier__max_depth\": [3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    5\n",
       "3    5\n",
       "4    5\n",
       "5    5\n",
       "6    5\n",
       "7    5\n",
       "8    5\n",
       "9    5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_other2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_other2, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7.011776\n",
       "1    7.011776\n",
       "2    7.011776\n",
       "3    7.011776\n",
       "4    0.221865\n",
       "5    0.221865\n",
       "6    0.221865\n",
       "7    0.221865\n",
       "8    0.221865\n",
       "9    0.221865\n",
       "dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "weights = compute_sample_weight(class_weight='balanced', y=y_other2)\n",
    "pd.Series(weights).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set #1\n",
      "    Test Set Size: 37513\n",
      "\n",
      "         Random State: 0\n",
      "\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV 1/2; 1/1] START xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4\n",
      "[CV 1/2; 1/1] END xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.550, test=0.219) total time= 2.9min\n",
      "[CV 2/2; 1/1] START xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4\n",
      "[CV 2/2; 1/1] END xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.521, test=0.214) total time= 3.3min\n",
      "best model parameters: {'xgbclassifier__learning_rate': 0.01, 'xgbclassifier__max_depth': 3, 'xgbclassifier__missing': nan, 'xgbclassifier__subsample': 0.4}\n",
      "\n",
      "validation score: 0.21640490585654448\n",
      "\n",
      "Test Set #2\n",
      "    Test Set Size: 37513\n",
      "\n",
      "         Random State: 0\n",
      "\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "[CV 1/2; 1/1] START xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4\n",
      "[CV 1/2; 1/1] END xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.472, test=0.171) total time= 3.6min\n",
      "[CV 2/2; 1/1] START xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4\n",
      "[CV 2/2; 1/1] END xgbclassifier__learning_rate=0.01, xgbclassifier__max_depth=3, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.507, test=0.183) total time= 3.6min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# splitter for subsampled data\n",
    "stratGroupKFold2 = StratifiedGroupKFold(n_splits=2)\n",
    "\n",
    "# splitter for other\n",
    "stratGroupKFold3 = StratifiedGroupKFold(n_splits=2)\n",
    "\n",
    "# label encoder for XGBoost\n",
    "le = LabelEncoder()\n",
    "y_subsample2 = le.fit_transform(y_subsample2)\n",
    "\n",
    "# Initialize lists\n",
    "nr_states = [0]\n",
    "test_scores = []\n",
    "final_models = []\n",
    "best_params = []\n",
    "test_scores = []\n",
    "counter = 0\n",
    "\n",
    "# num of lists should equal number of test sets that the StratifiedGroupKFold will generate\n",
    "# each list should contain entries with the same number of random state\n",
    "    \n",
    "list1_params = []\n",
    "list1_grid = []\n",
    "list1_pred = []\n",
    "list1_score = []\n",
    "\n",
    "list2_params = []\n",
    "list2_grid = []\n",
    "list2_pred = []\n",
    "list2_score = []\n",
    "\n",
    "list3_params = []\n",
    "list3_grid = []\n",
    "list3_pred = []\n",
    "list3_score = []\n",
    "\n",
    "for i_other2,i_test2 in stratGroupKFold2.split(X_subsample2.values, y_subsample2, groups_subsample2.values):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    \n",
    "    X_other2, y_other2, groups_other2 = X_subsample2.values[i_other2], y_subsample2[i_other2], groups_subsample2.values[i_other2]\n",
    "    X_test2, y_test2, groups_test2 = X_subsample2.values[i_test2], y_subsample2[i_test2], groups_subsample2.values[i_test2]\n",
    "\n",
    "    # Reshape the data\n",
    "    \n",
    "    X_other2 = pd.DataFrame(X_other2)\n",
    "    X_other2.columns = X_subsample2.columns\n",
    "\n",
    "    y_other2 = pd.DataFrame(y_other2)\n",
    "    y_other2.columns = y_subsample2_columns\n",
    "    \n",
    "\n",
    "    groups_other2 = pd.DataFrame(groups_other2)\n",
    "    groups_other2.columns = groups_subsample2.columns\n",
    "\n",
    "    y_other2 = np.reshape(np.array(y_other2), (1, -1)).ravel()\n",
    "\n",
    "    y_test2 = np.reshape(np.array(y_test2), (1, -1)).ravel()\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'Test Set #{counter}')\n",
    "\n",
    "    print(\"    Test Set Size:\", len(y_test2))\n",
    "\n",
    "    print()\n",
    "    \n",
    "    for i in range(len(nr_states)):\n",
    "\n",
    "        print(\"         Random State:\", i)\n",
    "        print()\n",
    "\n",
    "        # Perform n-Fold CV\n",
    "        cv = stratGroupKFold3.split(X_other2, y_other2, groups_other2)\n",
    "\n",
    "\n",
    "        # Initialize XGBoost Classifier\n",
    "        clf = xgb.XGBClassifier(num_class=10, eval_metric = 'mlogloss', \n",
    "                                random_state=i, use_label_encoder=False, )\n",
    "\n",
    "        pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid=param_grid1,scoring ='accuracy', \n",
    "                            cv=cv, return_train_score = True, n_jobs=1, verbose=10)\n",
    "        \n",
    "        # Compute sample weights\n",
    "        weights = compute_sample_weight(class_weight='balanced', y = y_other2)\n",
    "\n",
    "        grid_result = grid.fit(X_other2, y_other2, groups = groups_other2,\n",
    "                              xgbclassifier__sample_weight = weights) #xgbclassifier__early_stopping_rounds=50\n",
    "        \n",
    "        print('best model parameters:', grid.best_params_)\n",
    "\n",
    "        print()\n",
    "\n",
    "        print('validation score:',grid.best_score_)\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        means = grid_result.cv_results_['mean_test_score']\n",
    "        stds = grid_result.cv_results_['std_test_score']\n",
    "        \n",
    "        y_test_pred = grid.predict(X_test2)\n",
    "        score = accuracy_score(y_test,y_test_pred)\n",
    "        \n",
    "        if counter == 1:\n",
    "            list1_params.append(grid.best_params_)\n",
    "\n",
    "            list1_grid.append(grid)\n",
    "\n",
    "            list1_pred.append(y_test_pred)\n",
    "            \n",
    "            list1_score.append(score)\n",
    "\n",
    "        if counter == 2:\n",
    "            list2_params.append(grid.best_params_)\n",
    "\n",
    "            list2_grid.append(grid)\n",
    "            \n",
    "            list2_pred.append(y_test_pred)\n",
    "            \n",
    "            list2_score.append(score)\n",
    "            \n",
    "        if counter == 2:\n",
    "            list3_params.append(grid.best_params_)\n",
    "\n",
    "            list3_grid.append(grid)\n",
    "\n",
    "            list3_pred.append(y_test_pred)\n",
    "\n",
    "            list3_score.append(score)\n",
    "            \n",
    "            \n",
    "    \n",
    "        #print('test score:', score)\n",
    "\n",
    "        #test_scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "stratGroupKFold2 = StratifiedGroupKFold(n_splits=5)\n",
    "stratGroupKFold3 = StratifiedGroupKFold(n_splits=2)\n",
    "\n",
    "#clf = Pipeline(steps=[('preprocessor', preprocessor)])                                               \n",
    "counter = 0\n",
    "for i_other2,i_test2 in stratGroupKFold2.split(X_subsample2.values, y_subsample2.values, groups_subsample2.values):\n",
    "    \n",
    "    counter = counter + 1\n",
    "    print(f'Test Set #{counter}')\n",
    "\n",
    "    \n",
    "    X_other2, y_other2, groups_other2 = X_subsample2.values[i_other2], y_subsample2.values[i_other2], groups_subsample2.values[i_other2]\n",
    "    X_test2, y_test2, groups_test2 = X_subsample2.values[i_test2], y_subsample2.values[i_test2], groups_subsample2.values[i_test2]\n",
    "\n",
    "    #print(\"Size of OTHER:\", len(y_other))\n",
    "    print(\"Size of OTHER:\", len(y_other2))\n",
    "    print(\"Size of OTHER:\", len(y_other2)/len(y_subsample2))\n",
    "    print()\n",
    "\n",
    "    #print(\"Size of TEST:\", len(y_test))\n",
    "    print(\"Size of TEST:\", len(y_test2))\n",
    "    print(\"Size of TEST:\", len(y_test2)/len(y_subsample2))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_other2 = pd.DataFrame(X_other2)\n",
    "X_other2.columns = X_subsample2.columns\n",
    "\n",
    "y_other2 = pd.DataFrame(y_other2)\n",
    "y_other2.columns = y_subsample2.columns\n",
    "\n",
    "groups_other2 = pd.DataFrame(groups_other2)\n",
    "groups_other2.columns = groups_subsample2.columns\n",
    "\n",
    "y_other2 = np.reshape(np.array(y_other2), (1, -1)).ravel()\n",
    "\n",
    "y_test2 = np.reshape(np.array(y_test2), (1, -1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 391 Âµs, sys: 7.9 ms, total: 8.29 ms\n",
      "Wall time: 19.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cv = stratGroupKFold2.split(X_other2, y_other2, groups_other2)\n",
    "\n",
    "clf = xgb.XGBClassifier(use_label_encoder=False) # use_label_encoder=False\n",
    "\n",
    "pipe = make_pipeline(preprocessor,clf)\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid,scoring ='accuracy', \n",
    "                        cv=cv, return_train_score = True, n_jobs=1, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_other2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:57:47] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/4] END xgbclassifier__max_depth=1, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.676, test=0.446) total time=15.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:13:18] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/4] END xgbclassifier__max_depth=1, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.693, test=0.498) total time=15.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:28:46] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/4] END xgbclassifier__max_depth=1, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.696, test=0.466) total time=15.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:43:49] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/4] END xgbclassifier__max_depth=1, xgbclassifier__missing=nan, xgbclassifier__subsample=0.4;, score=(train=0.678, test=0.469) total time=15.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/francescaabulencia/opt/anaconda3/envs/data1030/lib/python3.10/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:59:39] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Best: 0.469953 using {'xgbclassifier__max_depth': 1, 'xgbclassifier__missing': nan, 'xgbclassifier__subsample': 0.4}\n",
      "CPU times: user 6h 59s, sys: 1h 45min 55s, total: 7h 46min 55s\n",
      "Wall time: 1h 31min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_result = grid.fit(X_other2, y_other2, groups=groups_other2)\n",
    "\n",
    "#results = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "#means = grid_result.cv_results_['mean_test_score']\n",
    "#stds = grid_result.cv_results_['std_test_score']\n",
    "#params = grid_result.cv_results_['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1h 31min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46995263])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = grid_result.cv_results_['mean_test_score']\n",
    "means"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data1030]",
   "language": "python",
   "name": "conda-env-data1030-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
